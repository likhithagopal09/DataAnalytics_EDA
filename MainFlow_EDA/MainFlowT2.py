# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wTFM6Yt-mJJGM9VzYenmCNCciUWN3EvX
"""

# Step 0: Setup

!pip -q install pandas numpy matplotlib seaborn scipy

import warnings
warnings.filterwarnings("ignore")

import os, io, textwrap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from google.colab import files

plt.rcParams["figure.figsize"] = (8,5)
plt.rcParams["axes.grid"] = True

# Create output folders
os.makedirs("plots", exist_ok=True)
os.makedirs("outputs", exist_ok=True)

# =========================
# Step 1: Upload & Load CSV (with encoding fix)
# =========================
from google.colab import files
import io
import pandas as pd

print("Upload your dataset CSV (e.g., Global_Superstore2.csv)")
uploaded = files.upload()

fname = next(iter(uploaded))

# Try utf-8, fallback to latin1 (works for Global Superstore datasets)
try:
    df_raw = pd.read_csv(io.BytesIO(uploaded[fname]), encoding="utf-8")
except UnicodeDecodeError:
    df_raw = pd.read_csv(io.BytesIO(uploaded[fname]), encoding="latin1")

# Try parsing date columns automatically
def try_parse_dates(df):
    for c in df.columns:
        if "date" in c.lower():
            df[c] = pd.to_datetime(df[c], errors="coerce")
    return df

df = try_parse_dates(df_raw.copy())
print("Shape:", df.shape)
display(df.head())

# =========================
# Step 2: Basic Info & Missingness
# =========================
print("dtypes:\n", df.dtypes)
print("\nMissing values per column:\n", df.isna().sum())
print("\nDuplicate rows:", df.duplicated().sum())

# =========================
# Step 3: Remove Duplicates
# =========================
before = len(df)
df = df.drop_duplicates()
dups_removed = before - len(df)
print("Duplicates removed:", dups_removed, "| New shape:", df.shape)

# =========================
# Step 4: Fill Missing Values
#   - numeric -> median
#   - categorical -> mode
# =========================
num_cols = df.select_dtypes(include="number").columns.tolist()
cat_cols = df.select_dtypes(exclude=["number","datetime"]).columns.tolist()

missing_before = df.isna().sum().sum()

for c in num_cols:
    df[c] = df[c].fillna(df[c].median())

for c in cat_cols:
    if df[c].isna().any():
        df[c] = df[c].fillna(df[c].mode().iloc[0])

# keep datetime NaT as-is (often meaningful); drop rows if you prefer:
# for c in df.select_dtypes(include="datetime").columns:
#     df = df[~df[c].isna()]

missing_after = df.isna().sum().sum()
print(f"Missing filled: {missing_before - missing_after} | Remaining missing: {missing_after}")

# =========================
# Step 5: Detect & Handle Outliers (IQR method)
#   Removes rows outside [Q1-1.5*IQR, Q3+1.5*IQR] for each numeric col
# =========================
def remove_outliers_iqr(dataframe, cols, factor=1.5):
    dfc = dataframe.copy()
    outlier_counts = {}
    for col in cols:
        if dfc[col].nunique() < 2:  # skip constant columns
            outlier_counts[col] = 0
            continue
        Q1, Q3 = dfc[col].quantile([0.25, 0.75])
        IQR = Q3 - Q1
        lower, upper = Q1 - factor*IQR, Q3 + factor*IQR
        before = len(dfc)
        dfc = dfc[(dfc[col].between(lower, upper)) | (dfc[col].isna())]
        outlier_counts[col] = before - len(dfc)
    return dfc, outlier_counts

df_no_out, out_stats = remove_outliers_iqr(df, num_cols, factor=1.5)
print("Rows before:", len(df), "| after:", len(df_no_out), "| removed:", len(df) - len(df_no_out))
print("Outliers removed by column:", out_stats)

# Use the outlier-removed set going forward
df = df_no_out.reset_index(drop=True)

# =========================
# Step 6: Statistical Analysis
# =========================
desc = df[num_cols].agg(["mean","median","std","var"]).T
display(desc)

corr = df[num_cols].corr(numeric_only=True)
display(corr.style.background_gradient(cmap="coolwarm"))

# =========================
# Step 7: Visualizations
#   - Histograms for numeric features
#   - Boxplots for numeric features
#   - Correlation Heatmap
# =========================

# Histograms
for c in num_cols:
    plt.figure()
    df[c].hist(bins=30)
    plt.title(f"Histogram - {c}")
    plt.xlabel(c); plt.ylabel("Count")
    plt.tight_layout()
    plt.savefig(f"plots/hist_{c}.png", dpi=150)
    plt.show()

# Boxplots
for c in num_cols:
    plt.figure()
    sns.boxplot(x=df[c], orient="h")
    plt.title(f"Boxplot - {c}")
    plt.tight_layout()
    plt.savefig(f"plots/box_{c}.png", dpi=150)
    plt.show()

# Heatmap
plt.figure(figsize=(8,6))
sns.heatmap(corr, annot=False, cmap="coolwarm", fmt=".2f", square=True)
plt.title("Correlation Heatmap")
plt.tight_layout()
plt.savefig("plots/correlation_heatmap.png", dpi=150)
plt.show()

# =========================
# Step 8: Optional domain visuals if columns exist (Sales/Profit/Region/Category)
# =========================
if set(["Sales","Profit"]).issubset(df.columns):
    plt.figure()
    sns.scatterplot(data=df, x="Sales", y="Profit")
    plt.title("Sales vs Profit")
    plt.tight_layout()
    plt.savefig("plots/sales_vs_profit.png", dpi=150)
    plt.show()

if "Region" in df.columns and "Sales" in df.columns:
    plt.figure()
    df.groupby("Region")["Sales"].sum().sort_values().plot(kind="barh")
    plt.title("Sales by Region")
    plt.tight_layout()
    plt.savefig("plots/sales_by_region.png", dpi=150)
    plt.show()

if "Category" in df.columns and "Sales" in df.columns:
    plt.figure()
    df.groupby("Category")["Sales"].sum().sort_values().plot(kind="barh")
    plt.title("Sales by Category")
    plt.tight_layout()
    plt.savefig("plots/sales_by_category.png", dpi=150)
    plt.show()

# =========================
# Step 9: Save Cleaned Data & Auto Summary Report
# =========================
df.to_csv("outputs/cleaned_dataset.csv", index=False)

# Top correlations (absolute) excluding self-corr
top_corr = (
    corr.where(~np.eye(corr.shape[0], dtype=bool))
        .abs()
        .stack()
        .sort_values(ascending=False)
        .head(10)
)

report = []
report.append(f"# EDA Summary\n")
report.append(f"- Original shape: {df_raw.shape}")
report.append(f"- After duplicate removal: {df_raw.drop_duplicates().shape}")
report.append(f"- Final shape after outliers: {df.shape}")
report.append(f"- Duplicates removed: {dups_removed}")
report.append(f"- Remaining missing cells: {df.isna().sum().sum()}")
report.append(f"\n## Numeric Columns\n- {', '.join(num_cols) if num_cols else 'None'}")
report.append("\n## Descriptive Stats (mean/median/std/var)\n")
report.append(desc.to_markdown())
report.append("\n## Top 10 Absolute Correlations (pairwise)\n")
report.append(top_corr.to_frame("abs_corr").to_markdown())

# Optional domain-specific snippets
if {"Sales","Profit"}.issubset(df.columns):
    sales_sum = df["Sales"].sum()
    profit_sum = df["Profit"].sum()
    report.append(f"\n- Total Sales: {sales_sum:,.2f}")
    report.append(f"- Total Profit: {profit_sum:,.2f}")

if "Region" in df.columns and "Sales" in df.columns:
    top_region = df.groupby("Region")["Sales"].sum().sort_values(ascending=False).head(3)
    report.append("\n### Top Regions by Sales\n")
    report.append(top_region.to_markdown())

if "Category" in df.columns and "Sales" in df.columns:
    top_cat = df.groupby("Category")["Sales"].sum().sort_values(ascending=False).head(5)
    report.append("\n### Top Categories by Sales\n")
    report.append(top_cat.to_markdown())

with open("outputs/EDA_summary.md","w") as f:
    f.write("\n".join(report))

print("Saved:")
print("- outputs/cleaned_dataset.csv")
print("- outputs/EDA_summary.md")
print("- plots/*.png")

# =========================
# Step 6: Predictive Modeling — Linear Regression for Sales
#   Features: Profit, Discount
# =========================
features = ["Profit","Discount"]
target = "Sales"

# Guard for missing columns
for c in features + [target]:
    assert c in df.columns, f"Missing required column: {c}"

X = df[features].copy()
y = df[target].astype(float)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2  = r2_score(y_test, y_pred)

print("Coefficients (Profit, Discount):", model.coef_)
print("Intercept:", model.intercept_)
print(f"MSE: {mse:.4f} | R²: {r2:.4f}")

# Save model
joblib.dump(model, "sales_outputs/linear_regression_sales.pkl")

# Predicted vs Actual plot
plt.figure()
plt.scatter(y_test, y_pred)
plt.title("Predicted vs Actual Sales")
plt.xlabel("Actual Sales"); plt.ylabel("Predicted Sales")
# Reference line
lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]
plt.plot(lims, lims)
plt.tight_layout()
plt.savefig("sales_plots/predicted_vs_actual.png", dpi=150)
plt.show()

# =========================
# Step 7: Insights & Simple Recommendations (Auto)
# =========================
lines = []
lines.append("# Sales Insights & Recommendations\n")

# --- Compute aggregates ---
sales_by_region = df.groupby("Region")["Sales"].sum()
sales_by_cat = df.groupby("Category")["Sales"].sum()

# --- Top regions & categories ---
top_regions = sales_by_region.sort_values(ascending=False).head(5)
top_cats    = sales_by_cat.sort_values(ascending=False).head(5)

lines.append("## Top Regions by Sales\n")
lines.append(top_regions.to_frame("Sales").to_markdown())
lines.append("\n## Top Categories by Sales\n")
lines.append(top_cats.to_frame("Sales").to_markdown())

# --- Correlations ---
corr_discount_sales = df["Discount"].corr(df["Sales"])
corr_profit_sales   = df["Profit"].corr(df["Sales"])
lines.append(f"\n- Corr(Discount, Sales): {corr_discount_sales:.3f}")
lines.append(f"- Corr(Profit, Sales): {corr_profit_sales:.3f}")

# --- Discount buckets (avg sales per discount range) ---
bins = pd.interval_range(start=df["Discount"].min(), end=df["Discount"].max(), periods=5)
disc_bucket = pd.cut(df["Discount"], bins=bins, include_lowest=True)
avg_sales_by_disc = df.groupby(disc_bucket)["Sales"].mean().sort_index()
lines.append("\n## Avg Sales by Discount Bucket\n")
lines.append(avg_sales_by_disc.to_frame("AvgSales").to_markdown())

# --- Model Performance (from Step 6) ---
lines.append("\n## Model Performance\n")
lines.append(f"- MSE: {mse:.4f}")
lines.append(f"- R²: {r2:.4f}")
lines.append(f"- Coefficients [Profit, Discount]: {model.coef_}")
lines.append(f"- Intercept: {model.intercept_}")

# --- Save insights report ---
with open("sales_outputs/sales_insights.md","w") as f:
    f.write("\n".join(lines))

print("Saved:")
print("- sales_outputs/sales_insights.md")